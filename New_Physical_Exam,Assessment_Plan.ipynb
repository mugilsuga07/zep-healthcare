{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PN4DAyqGViw8",
        "outputId": "ee4fb0c3-eb98-49cd-af05-0d181bb60520"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.44)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.44 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.44)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.22)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.23)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (4.12.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.44->llama-index) (2.7.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.44->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.44->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.44->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.44->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.44->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.44->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.44->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.44->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.44->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.44->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.44->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.44->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.44->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.44->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.44->llama-index) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.44->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.44->llama-index) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.44->llama-index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade llama-index\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SxPTfIJYzAit",
        "outputId": "d1d5da15-dd7f-4eab-d710-5f4c1a115cb9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.44)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.44 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.44)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.22)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.23)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.4)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.27.0)\n",
            "Requirement already satisfied: llamaindex-py-client<0.2.0,>=0.1.18 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.1.19)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.33.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (8.3.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (4.12.1)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.44->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse<0.5.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse<0.2.0,>=0.1.2->llama-index) (0.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.44->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.44->llama-index) (2.7.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.44->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.44->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.44->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.44->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.44->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.44->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.44->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.44->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.44->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.44->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.44->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.44->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.44->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.44->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.44->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.44->llama-index) (24.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.44->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llamaindex-py-client<0.2.0,>=0.1.18->llama-index-core==0.10.44->llama-index) (2.18.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.44->llama-index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HTnLQe8zc0o",
        "outputId": "a091854c-ae10-4cd2-ceab-87c1cf58a2ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.3-py3-none-any.whl (974 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m974.0/974.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_core-0.2.5-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.7/314.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.1-py3-none-any.whl (23 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.76-py3-none-any.whl (124 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.7.3)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: packaging, orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.0\n",
            "    Uninstalling packaging-24.0:\n",
            "      Successfully uninstalled packaging-24.0\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.3 langchain-core-0.2.5 langchain-text-splitters-0.2.1 langsmith-0.1.76 orjson-3.10.4 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s3bw2pfgzgiI",
        "outputId": "48c88035-1a06-472a-bd8d-07391844adfc"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.30)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.5)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.76)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.3.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (0.2.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.0->langchain_community) (2.7.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.0->langchain_community) (23.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.0->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.0->langchain_community) (2.18.4)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: langchain_community\n",
            "Successfully installed langchain_community-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "#OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# Hpisummary class\n",
        "class Hiisummary(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieving documents from the index\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combining the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['Alternatives'][0]['Content'] for item in transcript_data['Conversation']['TranscriptItems'] if 'Alternatives' in item and item['Alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Retrieving relevant knowledge base entries\n",
        "        relevant_knowledge_base = self.search_index(transcript_text)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "    input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "    template=(\n",
        "        \"The following is a clinical conversation between a doctor and a patient. \"\n",
        "        \"Based on this conversation transcript and using relevant information from the provided knowledgebase, \"\n",
        "        \"generate a detailed History of Present Illness (HPI) for the patient. The HPI should include the following sections:\\n\\n\"\n",
        "        \"1. Chief Complaint: Brief description of only the primary symptom.Dont mention the age of the patient.\\n\"\n",
        "        \"2. Previous Occurrences: Any similar past episodes.\\n\"\n",
        "        \"3. Family History: Relevant family medical history.\\n\"\n",
        "        \"4. Symptoms: only current symptoms.\\n\\n\"\n",
        "        \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "        \"Relevant Knowledge Base:\\n{knowledge_base}\\n\\n\"\n",
        "        \"Please ensure the HPI is comprehensive, clear, and formatted properly.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text, knowledge_base=relevant_knowledge_base)\n",
        "        summary = llm(prompt)\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = Hiisummary(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "\n",
        "transcript_path = '/content/transcript.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofQMPVd88UBI",
        "outputId": "3308f731-52c5-4a81-9331-65fefe44c4b8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-d13ec98dc1e7>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAI`.\n",
            "  warn_deprecated(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "History of Present Illness (HPI):\n",
            "- Chief Complaint: Stomach pains\n",
            "- Previous Occurrences: Last night, after eating Chinese food, patient experienced stomach pain which started in the middle and moved to the right. Patient also experienced stomach pain as a child when her appendix was taken out.\n",
            "- Family History: Patient's mother had similar gallbladder attacks after eating specific foods.\n",
            "- Symptoms: Current symptoms include stomach pain, nausea, and diarrhea.\n",
            "- Onset: Symptoms started last night after eating Chinese food.\n",
            "- Location: Stomach pain initially in the middle, now located in the right hypogastric and right upper quadrant area.\n",
            "- Character: Pain described as a twinge at first, but has worsened throughout the day.\n",
            "- Severity: Pain initially rated as a 6 out of 10, but has worsened and is now a 6 out of 10.\n",
            "- Associated symptoms: Nausea and diarrhea.\n",
            "- Timing: Symptoms started last night and have worsened throughout the day.\n",
            "- Modifying factors: Taking Tums initially provided some relief, but pain has worsened despite taking Tums.\n",
            "- Context: Patient's mother has a history of gallbladder attacks.\n",
            "- Duration: Symptoms have been ongoing since last\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ROS"
      ],
      "metadata": {
        "id": "lcppysr1zmWM"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# Hpisummary class\n",
        "class Hiisummary(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieve documents from the index using the query\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combine the texts of the top results, limiting the total length\n",
        "        combined_text = \"\"\n",
        "        for result in search_results:\n",
        "            combined_text += result.text.replace(\"Patient\", \"Other Patient\") + \"\\n\"\n",
        "            if len(combined_text.split()) > 3000:  # Adjust this limit as needed\n",
        "                break\n",
        "\n",
        "        return combined_text\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['Alternatives'][0]['Content'] for item in transcript_data['Conversation']['TranscriptItems'] if 'Alternatives' in item and item['Alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Retrieving relevant knowledge base entries\n",
        "        relevant_knowledge_base = self.search_index(transcript_text)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "            template=(\n",
        "                \"Based on the provided clinical conversation and the Conversation Transcript, generate a detailed Review of Systems (RoS) for the patient based on Conversation Transcript. \"\n",
        "                \"The RoS should cover the following body systems and include specific details if mentioned, otherwise state 'denies any issues':\\n\\n\"\n",
        "                \"Write Ros based on the Conversation Transcript.\"\n",
        "\n",
        "                \"1. General: Chief complaint\\n\"\n",
        "                \"2. Skin: Any skin issues or rashes.\\n\"\n",
        "                \"3. Head: Any headaches or head injuries.\\n\"\n",
        "                \"4. Eyes: Vision problems or eye pain.\\n\"\n",
        "                \"5. Ears, Nose, Throat: Hearing issues, nasal congestion, sore throat, etc.\\n\"\n",
        "                \"6. Cardiovascular: Heart problems, chest pain, etc.\\n\"\n",
        "                \"7. Respiratory: Breathing issues, cough, etc.\\n\"\n",
        "                \"8. Gastrointestinal: Abdominal pain, nausea, etc.\\n\"\n",
        "                \"9. Genitourinary: Urination problems, etc.\\n\"\n",
        "                \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                \"Knowledge Base,if only  any reference is present:\\n{knowledge_base}\\n\\n\"\n",
        "                \"Please ensure the RoS is comprehensive, clear, and formatted properly.\"\n",
        "                \"Please go through the entire Conversation Transcript and mention yes only if the pain is present in the Conversation Transcript.\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text, knowledge_base=relevant_knowledge_base)\n",
        "        summary = llm(prompt)\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = Hiisummary(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/transcript.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLcii_aQsPqz",
        "outputId": "210bbdea-3e8c-4dd8-8f10-dc36d37a16e9"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-37-2f4d3b1b2a1b>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. General: Presents with stomach pain, nausea, bloating, and diarrhea. Denies any fever or chills.\n",
            "2. Skin: Denies any skin issues or rashes.\n",
            "3. Head: Denies any headaches or head injuries.\n",
            "4. Eyes: Denies any vision problems or eye pain.\n",
            "5. Ears, Nose, Throat: Denies any hearing issues, nasal congestion, or sore throat.\n",
            "6. Cardiovascular: Denies any heart problems or chest pain.\n",
            "7. Respiratory: Denies any breathing issues or cough.\n",
            "8. Gastrointestinal: Presents with abdominal pain, nausea, bloating, and diarrhea. Denies any changes in stool color or blood in stool.\n",
            "9. Genitourinary: Denies any urinary symptoms.\n",
            "10. Musculoskeletal: Presents with joint pain all over the body. No history of previous joint pain or injury mentioned.\n",
            "11. Neurological: Denies any headaches, changes in vision, difficulty swallowing, weakness, slurred speech, confusion, or leg pain.\n",
            "12. Integumentary: Denies any skin issues or rashes.\n",
            "13. Endocrine: Denies any history of diabetes or thyroid problems.\n",
            "14. Psychiatric: Denies any history of mental health issues\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW MA"
      ],
      "metadata": {
        "id": "XgtLRQUQZCBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# Hpisummary class\n",
        "class Hiisummary(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieve documents from the index using the query\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combine the texts of the top results, limiting the total length\n",
        "        combined_text = \"\"\n",
        "        for result in search_results:\n",
        "            combined_text += result.text.replace(\"Patient\", \"Other Patient\") + \"\\n\"\n",
        "            if len(combined_text.split()) > 3000:  # Adjust this limit as needed\n",
        "                break\n",
        "\n",
        "        return combined_text\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['Alternatives'][0]['Content'] for item in transcript_data['Conversation']['TranscriptItems'] if 'Alternatives' in item and item['Alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Retrieving relevant knowledge base entries\n",
        "        relevant_knowledge_base = self.search_index(transcript_text)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "    input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "    template=(\n",
        "        \"Based on the provided clinical conversation and the following transcript, generate a detailed Review of Systems (RoS) for the patient. \"\n",
        "        \"The RoS should cover the following body systems and include specific details only if they are mentioned in the transcript. \"\n",
        "        \"If a system is not mentioned, do not include it in the RoS:\\n\\n\"\n",
        "        \"1. General: Chief complaint.\\n\"\n",
        "        \"2. Skin: Any skin issues or rashes.\\n\"\n",
        "        \"3. Head: Any headaches or head injuries.\\n\"\n",
        "        \"4. Eyes: Vision problems or eye pain.\\n\"\n",
        "        \"5. Ears, Nose, Throat: Hearing issues, nasal congestion, sore throat.\\n\"\n",
        "        \"6. Cardiovascular: Heart problems, chest pain.\\n\"\n",
        "        \"7. Respiratory: Breathing issues, cough.\\n\"\n",
        "        \"8. Gastrointestinal: Abdominal pain, nausea.\\n\"\n",
        "        \"9. Genitourinary: Urination problems.\\n\"\n",
        "        \"10. Musculoskeletal: Muscle pain, joint pain, etc.\\n\"\n",
        "        \"11. Neurological: Numbness, tingling, etc.\\n\"\n",
        "        \"12. Psychiatric: Depression, anxiety, etc.\\n\\n\"\n",
        "        \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "        \"Please ensure the RoS is comprehensive, clear, and formatted properly. Only include the systems that are explicitly mentioned in the transcript.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text, knowledge_base=relevant_knowledge_base)\n",
        "        summary = llm(prompt)\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = Hiisummary(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/transcript.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZaEqafAZDRJ",
        "outputId": "7da89d14-3960-4154-847a-3f6769c1c359"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-61-6332f8caab06>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Review of Systems (RoS) for Patient:\n",
            "\n",
            "1. General:\n",
            "- Chief complaint: Stomach pains, nausea, and diarrhea.\n",
            "\n",
            "2. Skin:\n",
            "- No mention of skin issues or rashes.\n",
            "\n",
            "3. Head:\n",
            "- No mention of headaches or head injuries.\n",
            "\n",
            "4. Eyes:\n",
            "- No mention of vision problems or eye pain.\n",
            "\n",
            "5. Ears, Nose, Throat:\n",
            "- No mention of hearing issues.\n",
            "- Nasal congestion: No.\n",
            "- Sore throat: No.\n",
            "\n",
            "6. Cardiovascular:\n",
            "- No mention of heart problems.\n",
            "- Chest pain: No.\n",
            "\n",
            "7. Respiratory:\n",
            "- No mention of breathing issues.\n",
            "- Cough: No.\n",
            "\n",
            "8. Gastrointestinal:\n",
            "- Abdominal pain: Yes, in the middle and then moved to the right.\n",
            "- Nausea: Yes, but no vomiting.\n",
            "- Diarrhea: Yes, but more like loose stool.\n",
            "- Changes in stool color: Lighter than normal.\n",
            "- Bloating: Yes, a little.\n",
            "- Urinary symptoms: No.\n",
            "\n",
            "9. Genitourinary:\n",
            "- Urination problems: No.\n",
            "\n",
            "10. Musculoskeletal:\n",
            "- No mention of muscle or joint pain.\n",
            "\n",
            "11. Neurological:\n",
            "- No mention of numbness or tingling.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PE"
      ],
      "metadata": {
        "id": "cgAsbeqf0B0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# TestNameExtractor class for extracting test names\n",
        "class TestNameExtractor(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieving documents from the index\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combining the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['Alternatives'][0]['Content'] for item in transcript_data['Conversation']['TranscriptItems'] if 'Alternatives' in item and item['Alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Retrieving relevant knowledge base entries\n",
        "        relevant_knowledge_base = self.search_index(transcript_text)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "    input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "    template=(\n",
        "        \"The following is a clinical conversation between a doctor and a patient. \"\n",
        "        \"Based on this conversation transcript and use only if relevant information from the provided knowledgebase, \"\n",
        "        \"extract only the names of the tests mentioned by the doctor during the examination present in the Conversation Transcript. \"\n",
        "        \"Do not include any medications or treatments, only list the tests.\\n\\n\"\n",
        "        \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "        \"Knowledge Base,if only  any reference is present:\\n{knowledge_base}\\n\\n\"\n",
        "        \"Please list only the test names mentioned by the doctor in the Conversation Transcript , excluding any medications or treatments.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text, knowledge_base=relevant_knowledge_base)\n",
        "        summary = llm(prompt)\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = TestNameExtractor(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/transcript.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8lq1tgq3rbQ",
        "outputId": "ab8d1e76-38a9-4a8e-f9a9-6f2effce16e7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-27-32430ca57b98>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. CBC (Complete Blood Count)\n",
            "2. Basic Metabolic Panel\n",
            "3. Liver Function Test\n",
            "4. Urinalysis\n",
            "5. EKG (Electrocardiogram)\n",
            "6. Chest X-ray\n",
            "7. Troponin\n",
            "8. CT Scan (Computed Tomography)\n",
            "9. Pregnancy Test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assessment Plan\n"
      ],
      "metadata": {
        "id": "eQ9aArbs407i"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# AssessmentPlan class for generating the assessment plan\n",
        "class AssessmentPlan(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieving documents from the index\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combining the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['Alternatives'][0]['Content'] for item in transcript_data['Conversation']['TranscriptItems'] if 'Alternatives' in item and item['Alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Retrieving relevant knowledge base entries\n",
        "        relevant_knowledge_base = self.search_index(transcript_text)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "    input_variables=[\"transcript\"],\n",
        "    template=(\n",
        "        \"Based on the provided clinical conversation between the doctor and the patient, generate a detailed Assessment Plan. \"\n",
        "        \"The Assessment Plan should include only the doctor's assessment and plan, clearly separated into two sections: Assessment and Plan. \"\n",
        "        \"Each section should be detailed and follow the structure provided in the conversation.\\n\\n\"\n",
        "        \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "        \"Assessment:\\n\"\n",
        "        \"Provide a detailed assessment of the patient's condition based on the Conversation Transcript.\\n\\n\"\n",
        "        \"Plan:\\n\"\n",
        "        \"1. List only the results of diagnostic tests given to the patient mentioned in Transcript Conversation.\\n\"\n",
        "        \"2. Include any relevant diagnostic results, such as EKG findings, if mentioned in the Transcript Conversation.\\n\"\n",
        "        \"3. Mention the diagnosis and follow-up instructions provided by the doctor in Transcript Conversation.\\n\"\n",
        "        \"Ensure not to include any medications or treatments that are not mentioned explicitly in the transcript.\"\n",
        "        \"Please ensure the assessment plan is comprehensive, clear, and formatted properly.\"\n",
        "    )\n",
        ")\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text, knowledge_base=relevant_knowledge_base)\n",
        "        summary = llm(prompt)\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = AssessmentPlan(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/transcript.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4DtPt7FmOGu",
        "outputId": "f593241b-5658-4dba-a14b-331a13e0c73a"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-121-6d27c6c21585>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Assessment:\n",
            "\n",
            "The patient presents with complaints of stomach pain that started after consuming fried Chinese food. The pain started in the center of the abdomen and has now moved to the right side. The patient also reports feeling nauseous and having some diarrhea. On a scale of 1 to 10, the patient rates the pain as a 6. The doctor notes a history of the patient having had her appendix removed as a child. The patient also reports not having any issues with fried food in the past.\n",
            "\n",
            "Plan:\n",
            "\n",
            "1. Diagnostic Tests:\n",
            "- CBC (Complete Blood Count)\n",
            "- Basic Metabolic Panel\n",
            "- Liver Function Test\n",
            "- Urinalysis\n",
            "- EKG (Electrocardiogram)\n",
            "- Chest X-ray\n",
            "- Troponin\n",
            "- CT Scan of the abdomen and pelvis\n",
            "- Pregnancy test\n",
            "- IV fluids (1 L of normal saline)\n",
            "\n",
            "2. Diagnostic Results:\n",
            "- EKG shows minor nonspecific ST segment changes in the lateral leads and J point elevation consistent with early repolarization.\n",
            "- Trace pitting edema noted on examination.\n",
            "\n",
            "3. Diagnosis:\n",
            "Based on the patient's symptoms and physical examination, the doctor suspects the patient may be experiencing biliary colic, which is characterized by abdominal pain, nausea, and vomiting after consuming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WXxl74zE9sjF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}