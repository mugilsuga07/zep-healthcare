{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OV7Tf2SYFDjT",
        "outputId": "ac390c47-46d1-4b4f-b888-a5d5fddf36f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.10.52-py3-none-any.whl (6.8 kB)\n",
            "Collecting llama-index-agent-openai<0.3.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_agent_openai-0.2.7-py3-none-any.whl (12 kB)\n",
            "Collecting llama-index-cli<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_cli-0.1.12-py3-none-any.whl (26 kB)\n",
            "Collecting llama-index-core==0.10.52.post1 (from llama-index)\n",
            "  Downloading llama_index_core-0.10.52.post1-py3-none-any.whl (15.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.4/15.4 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-embeddings-openai<0.2.0,>=0.1.5 (from llama-index)\n",
            "  Downloading llama_index_embeddings_openai-0.1.10-py3-none-any.whl (6.2 kB)\n",
            "Collecting llama-index-indices-managed-llama-cloud>=0.2.0 (from llama-index)\n",
            "  Downloading llama_index_indices_managed_llama_cloud-0.2.3-py3-none-any.whl (9.2 kB)\n",
            "Collecting llama-index-legacy<0.10.0,>=0.9.48 (from llama-index)\n",
            "  Downloading llama_index_legacy-0.9.48-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-index-llms-openai<0.2.0,>=0.1.13 (from llama-index)\n",
            "  Downloading llama_index_llms_openai-0.1.25-py3-none-any.whl (11 kB)\n",
            "Collecting llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_multi_modal_llms_openai-0.1.7-py3-none-any.whl (5.9 kB)\n",
            "Collecting llama-index-program-openai<0.2.0,>=0.1.3 (from llama-index)\n",
            "  Downloading llama_index_program_openai-0.1.6-py3-none-any.whl (5.2 kB)\n",
            "Collecting llama-index-question-gen-openai<0.2.0,>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_question_gen_openai-0.1.3-py3-none-any.whl (2.9 kB)\n",
            "Collecting llama-index-readers-file<0.2.0,>=0.1.4 (from llama-index)\n",
            "  Downloading llama_index_readers_file-0.1.27-py3-none-any.whl (37 kB)\n",
            "Collecting llama-index-readers-llama-parse>=0.1.2 (from llama-index)\n",
            "  Downloading llama_index_readers_llama_parse-0.1.6-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.9.5)\n",
            "Collecting dataclasses-json (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2023.6.0)\n",
            "Collecting httpx (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting llama-cloud<0.0.7,>=0.0.6 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading llama_cloud-0.0.6-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.8/130.8 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.25.2)\n",
            "Collecting openai>=1.1.0 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading openai-1.35.10-py3-none-any.whl (328 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.3/328.3 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (8.4.2)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (4.12.2)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Collecting pypdf<5.0.0,>=4.0.1 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.4/290.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting striprtf<0.0.27,>=0.0.26 (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index)\n",
            "  Downloading striprtf-0.0.26-py3-none-any.whl (6.9 kB)\n",
            "Collecting llama-parse>=0.4.0 (from llama-index-readers-llama-parse>=0.1.2->llama-index)\n",
            "  Downloading llama_parse-0.4.5-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.52.post1->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.52.post1->llama-index) (3.0.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->llama-index-core==0.10.52.post1->llama-index)\n",
            "  Downloading marshmallow-3.21.3-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.52.post1->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.52.post1->llama-index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.52.post1->llama-index) (1.16.0)\n",
            "Installing collected packages: striprtf, dirtyjson, pypdf, mypy-extensions, marshmallow, h11, deprecated, typing-inspect, tiktoken, httpcore, httpx, dataclasses-json, openai, llama-cloud, llama-index-legacy, llama-index-core, llama-parse, llama-index-readers-file, llama-index-llms-openai, llama-index-indices-managed-llama-cloud, llama-index-embeddings-openai, llama-index-readers-llama-parse, llama-index-multi-modal-llms-openai, llama-index-cli, llama-index-agent-openai, llama-index-program-openai, llama-index-question-gen-openai, llama-index\n",
            "Successfully installed dataclasses-json-0.6.7 deprecated-1.2.14 dirtyjson-1.0.8 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 llama-cloud-0.0.6 llama-index-0.10.52 llama-index-agent-openai-0.2.7 llama-index-cli-0.1.12 llama-index-core-0.10.52.post1 llama-index-embeddings-openai-0.1.10 llama-index-indices-managed-llama-cloud-0.2.3 llama-index-legacy-0.9.48 llama-index-llms-openai-0.1.25 llama-index-multi-modal-llms-openai-0.1.7 llama-index-program-openai-0.1.6 llama-index-question-gen-openai-0.1.3 llama-index-readers-file-0.1.27 llama-index-readers-llama-parse-0.1.6 llama-parse-0.4.5 marshmallow-3.21.3 mypy-extensions-1.0.0 openai-1.35.10 pypdf-4.2.0 striprtf-0.0.26 tiktoken-0.7.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade llama-index\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5IDX4lwTIlc2",
        "outputId": "9e3f88df-da45-47db-d5e9-f726c57f85bf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: llama-index in /usr/local/lib/python3.10/dist-packages (0.10.52)\n",
            "Requirement already satisfied: llama-index-agent-openai<0.3.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.7)\n",
            "Requirement already satisfied: llama-index-cli<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.12)\n",
            "Requirement already satisfied: llama-index-core==0.10.52.post1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.10.52.post1)\n",
            "Requirement already satisfied: llama-index-embeddings-openai<0.2.0,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.10)\n",
            "Requirement already satisfied: llama-index-indices-managed-llama-cloud>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.2.3)\n",
            "Requirement already satisfied: llama-index-legacy<0.10.0,>=0.9.48 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.9.48)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.13 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.25)\n",
            "Requirement already satisfied: llama-index-multi-modal-llms-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.7)\n",
            "Requirement already satisfied: llama-index-program-openai<0.2.0,>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: llama-index-question-gen-openai<0.2.0,>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.3)\n",
            "Requirement already satisfied: llama-index-readers-file<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.27)\n",
            "Requirement already satisfied: llama-index-readers-llama-parse>=0.1.2 in /usr/local/lib/python3.10/dist-packages (from llama-index) (0.1.6)\n",
            "Requirement already satisfied: PyYAML>=6.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (0.6.7)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy<2.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.25.2)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.35.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (9.4.0)\n",
            "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (8.4.2)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core==0.10.52.post1->llama-index) (1.14.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.12.3)\n",
            "Requirement already satisfied: pypdf<5.0.0,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (4.2.0)\n",
            "Requirement already satisfied: striprtf<0.0.27,>=0.0.26 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (0.0.26)\n",
            "Requirement already satisfied: llama-parse>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-readers-llama-parse>=0.1.2->llama-index) (0.4.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core==0.10.52.post1->llama-index) (4.0.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.12.3->llama-index-readers-file<0.2.0,>=0.1.4->llama-index) (2.5)\n",
            "Requirement already satisfied: pydantic>=1.10 in /usr/local/lib/python3.10/dist-packages (from llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (2.8.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core==0.10.52.post1->llama-index) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core==0.10.52.post1->llama-index) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core==0.10.52.post1->llama-index) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core==0.10.52.post1->llama-index) (1.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama-index) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31.0->llama-index-core==0.10.52.post1->llama-index) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core==0.10.52.post1->llama-index) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core==0.10.52.post1->llama-index) (1.0.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json->llama-index-core==0.10.52.post1->llama-index) (3.21.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core==0.10.52.post1->llama-index) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core==0.10.52.post1->llama-index) (1.2.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core==0.10.52.post1->llama-index) (24.1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.10->llama-cloud<0.0.7,>=0.0.6->llama-index-core==0.10.52.post1->llama-index) (2.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core==0.10.52.post1->llama-index) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pg6T7lU2IuEz",
        "outputId": "b308ba95-56e5-4e18-d93f-f5ab47307396"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.2.6-py3-none-any.whl (975 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m975.5/975.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.3.0,>=0.2.10 (from langchain)\n",
            "  Downloading langchain_core-0.2.11-py3-none-any.whl (337 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.4/337.4 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.2.2-py3-none-any.whl (25 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.84-py3-none-any.whl (127 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 kB\u001b[0m \u001b[31m408.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain) (24.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.6.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain-core, langchain-text-splitters, langchain\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.6 langchain-core-0.2.11 langchain-text-splitters-0.2.2 langsmith-0.1.84 orjson-3.10.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "471JugnVFPR9",
        "outputId": "8280b01d-1a70-400d-fa64-de95a0b557a5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.2.6-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.9.5)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.3.0,>=0.2.6 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.6)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.10 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.2.11)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.84)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.25.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.31.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.4.2)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.21.3)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (0.2.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.3.0,>=0.2.6->langchain_community) (2.8.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.10->langchain_community) (24.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain_community) (3.10.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2024.6.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.0.3)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.10->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.3.0,>=0.2.6->langchain_community) (2.20.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Installing collected packages: langchain_community\n",
            "Successfully installed langchain_community-0.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index-llms-langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EaOtF1mFOcc2",
        "outputId": "b9926520-499a-44ae-8819-95b1852b59c7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index-llms-langchain\n",
            "  Downloading llama_index_llms_langchain-0.1.4-py3-none-any.whl (4.8 kB)\n",
            "Collecting langchain<0.2.0,>=0.1.3 (from llama-index-llms-langchain)\n",
            "  Downloading langchain-0.1.20-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: llama-index-core<0.11.0,>=0.10.41 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-langchain) (0.10.52.post1)\n",
            "Collecting llama-index-llms-anyscale<0.2.0,>=0.1.1 (from llama-index-llms-langchain)\n",
            "  Downloading llama_index_llms_anyscale-0.1.4-py3-none-any.whl (4.2 kB)\n",
            "Requirement already satisfied: llama-index-llms-openai<0.2.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-llms-langchain) (0.1.25)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.0.31)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.9.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (4.0.3)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.6.7)\n",
            "Collecting langchain-community<0.1,>=0.0.38 (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
            "  Downloading langchain_community-0.0.38-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2.0,>=0.1.52 (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
            "  Downloading langchain_core-0.1.52-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.9/302.9 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
            "  Downloading langchain_text_splitters-0.0.2-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.1.84)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.8.0)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (8.4.2)\n",
            "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.2.14)\n",
            "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.0.8)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2023.6.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (0.27.0)\n",
            "Requirement already satisfied: llama-cloud<0.0.7,>=0.0.6 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (0.0.6)\n",
            "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.6.0)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (3.3)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (3.8.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.35.10)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2.0.3)\n",
            "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (9.4.0)\n",
            "Requirement already satisfied: tiktoken>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (0.7.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (4.12.2)\n",
            "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (0.9.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.14.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.9.4)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.21.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (1.33)\n",
            "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain)\n",
            "  Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.10.6)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2024.6.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.0.5)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (3.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (0.14.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2024.5.15)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.7.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (2.0.7)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.0.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.0.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (2024.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.2.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.52->langchain<0.2.0,>=0.1.3->llama-index-llms-langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->llama-index-core<0.11.0,>=0.10.41->llama-index-llms-langchain) (1.16.0)\n",
            "Installing collected packages: packaging, langchain-core, langchain-text-splitters, langchain-community, llama-index-llms-anyscale, langchain, llama-index-llms-langchain\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.1\n",
            "    Uninstalling packaging-24.1:\n",
            "      Successfully uninstalled packaging-24.1\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.2.11\n",
            "    Uninstalling langchain-core-0.2.11:\n",
            "      Successfully uninstalled langchain-core-0.2.11\n",
            "  Attempting uninstall: langchain-text-splitters\n",
            "    Found existing installation: langchain-text-splitters 0.2.2\n",
            "    Uninstalling langchain-text-splitters-0.2.2:\n",
            "      Successfully uninstalled langchain-text-splitters-0.2.2\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.2.6\n",
            "    Uninstalling langchain-community-0.2.6:\n",
            "      Successfully uninstalled langchain-community-0.2.6\n",
            "  Attempting uninstall: langchain\n",
            "    Found existing installation: langchain 0.2.6\n",
            "    Uninstalling langchain-0.2.6:\n",
            "      Successfully uninstalled langchain-0.2.6\n",
            "Successfully installed langchain-0.1.20 langchain-community-0.0.38 langchain-core-0.1.52 langchain-text-splitters-0.0.2 llama-index-llms-anyscale-0.1.4 llama-index-llms-langchain-0.1.4 packaging-23.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HPI"
      ],
      "metadata": {
        "id": "i3S8jLLRQx4q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# Hpisummary class\n",
        "class Hiisummary(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieving documents from the index\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combining the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def map_phase(self, text_chunks):\n",
        "        intermediate_summaries = []\n",
        "        for chunk in text_chunks:\n",
        "            relevant_knowledge_base = self.search_index(chunk)\n",
        "            prompt_template = PromptTemplate(\n",
        "                input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "                template=(\n",
        "                    \"The following is a clinical conversation between a doctor and a patient. \"\n",
        "                    \"Based on this conversation transcript and using only relevant information from the provided knowledgebase, \"\n",
        "                    \"generate a detailed History of Present Illness (HPI) for the patient. The HPI should include the following sections:\\n\\n\"\n",
        "                    \"1. Chief Complaint: Brief description of only the primary symptom. Don't mention the age of the patient.\\n\"\n",
        "                    \"2. Previous Occurrences: Any similar past episodes.\\n\"\n",
        "                    \"3. Family History: Relevant family medical history.\\n\"\n",
        "                    \"4. Symptoms: Only current symptoms.\\n\\n\"\n",
        "                    \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                    \"Relevant Knowledge Base:\\n{knowledge_base}\\n\\n\"\n",
        "                    \"Please ensure the HPI is comprehensive, clear, and formatted properly.\"\n",
        "                )\n",
        "            )\n",
        "            llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "            prompt = prompt_template.format(transcript=chunk, knowledge_base=relevant_knowledge_base)\n",
        "            try:\n",
        "                summary = llm(prompt)\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating summary for chunk: {chunk[:50]}...: {e}\")\n",
        "                summary = \"\"\n",
        "            intermediate_summaries.append(summary)\n",
        "        return intermediate_summaries\n",
        "\n",
        "    def reduce_phase(self, intermediate_summaries):\n",
        "        final_hpi = {\n",
        "            \"Chief Complaint\": set(),\n",
        "            \"Previous Occurrences\": set(),\n",
        "            \"Family History\": set(),\n",
        "            \"Symptoms\": set()\n",
        "        }\n",
        "\n",
        "        for summary in intermediate_summaries:\n",
        "            sections = summary.split(\"\\n\\n\")\n",
        "            for section in sections:\n",
        "                if section.startswith(\"Chief Complaint:\"):\n",
        "                    final_hpi[\"Chief Complaint\"].add(section.replace(\"Chief Complaint:\", \"\").strip())\n",
        "                elif section.startswith(\"Previous Occurrences:\"):\n",
        "                    final_hpi[\"Previous Occurrences\"].add(section.replace(\"Previous Occurrences:\", \"\").strip())\n",
        "                elif section.startswith(\"Family History:\"):\n",
        "                    final_hpi[\"Family History\"].add(section.replace(\"Family History:\", \"\").strip())\n",
        "                elif section.startswith(\"Symptoms:\"):\n",
        "                    final_hpi[\"Symptoms\"].add(section.replace(\"Symptoms:\", \"\").strip())\n",
        "\n",
        "        combined_summary = (\n",
        "            \"History of Present Illness (HPI):\\n\\n\"\n",
        "            f\"Chief Complaint: {' '.join(final_hpi['Chief Complaint'])}\\n\\n\"\n",
        "            f\"Previous Occurrences: {' '.join(final_hpi['Previous Occurrences'])}\\n\\n\"\n",
        "            f\"Family History: {' '.join(final_hpi['Family History'])}\\n\\n\"\n",
        "            f\"Symptoms: {' '.join(final_hpi['Symptoms'])}\"\n",
        "        )\n",
        "        return combined_summary\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['alternatives'][0]['content'] for item in transcript_data['results']['items'] if 'alternatives' in item and item['alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Splitting the transcript into chunks for Map phase\n",
        "        text_chunks = [transcript_text[i:i+2000] for i in range(0, len(transcript_text), 2000)]\n",
        "\n",
        "        # Map phase: generate intermediate summaries for each chunk\n",
        "        intermediate_summaries = self.map_phase(text_chunks)\n",
        "\n",
        "        # Reduce phase: combine intermediate summaries into final summary\n",
        "        final_summary = self.reduce_phase(intermediate_summaries)\n",
        "        return final_summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = Hiisummary(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "transcript_path = '/content/June12-01_AbdominalPain.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dVn9M0ZeL1jW",
        "outputId": "2b1aefe5-d2ce-435d-d62c-ac3ac9211bc1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-45-55da83e26bb9>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "History of Present Illness (HPI):\n",
            "\n",
            "Chief Complaint: Abdominal pain.\n",
            "\n",
            "Previous Occurrences: The patient, a 26-year-old pregnant female, presents with sharp abdominal pain, located under her right ribs. She has also experienced morning sickness and lower back pain in the past, but these symptoms have since resolved.\n",
            "\n",
            "Family History: The patient has a history of four living children and one previous miscarriage.\n",
            "\n",
            "Symptoms: The patient is currently experiencing sharp abdominal pain, morning sickness, and lower back pain. She denies any bleeding, discharge, or concerns for sexually transmitted diseases. She also denies any vomiting, other than the normal morning sickness associated with pregnancy. She is not taking any medication for her symptoms and has not yet seen an obstetrician-gynecologist (OBGYN).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#ROS"
      ],
      "metadata": {
        "id": "mfeCDzhUP9qN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# RoSsummary class\n",
        "class RoSsummary(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieve documents from the index using the query\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combine the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['alternatives'][0]['content'] for item in transcript_data['results']['items'] if 'alternatives' in item and item['alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Retrieving relevant knowledge base entries\n",
        "        relevant_knowledge_base = self.search_index(transcript_text)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "            template=(\n",
        "                \"Based on the provided clinical conversation and the Conversation Transcript, generate a detailed Review of Systems (RoS) for the patient based on the Conversation Transcript. \"\n",
        "                \"The RoS should cover the following body systems and include specific details if mentioned, otherwise state 'denies any issues':\\n\\n\"\n",
        "                \"1. General: Chief complaint\\n\"\n",
        "                \"2. Skin: Any skin issues or rashes.\\n\"\n",
        "                \"3. Head: Any headaches or head injuries.\\n\"\n",
        "                \"4. Eyes: Vision problems or eye pain.\\n\"\n",
        "                \"5. Ears, Nose, Throat: Hearing issues, nasal congestion, sore throat, etc.\\n\"\n",
        "                \"6. Cardiovascular: Heart problems, chest pain, etc.\\n\"\n",
        "                \"7. Respiratory: Breathing issues, cough, etc.\\n\"\n",
        "                \"8. Gastrointestinal: Abdominal pain, nausea, etc.\\n\"\n",
        "                \"9. Genitourinary: Urination problems, etc.\\n\"\n",
        "                \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                \"Relevant Knowledge Base:\\n{knowledge_base}\\n\\n\"\n",
        "                \"Please ensure the RoS is comprehensive, clear, and formatted properly.\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text, knowledge_base=relevant_knowledge_base)\n",
        "        summary = llm(prompt)\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = RoSsummary(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/June12-01_AbdominalPain.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvQ73tChXL7x",
        "outputId": "2c1a9ba8-2019-4290-bf1b-203fd536bc06"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-908a53369204>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Review of Systems (RoS) for the patient:\n",
            "\n",
            "1. General: The patient is a 9-week pregnant 53-year-old female who presents with sharp abdominal pain located under her right ribs. She also reports morning sickness and lower back pain that has resolved. She denies any bleeding, discharge, or concerns for STIs. She also denies any vomiting other than normal morning sickness.\n",
            "\n",
            "2. Skin: The patient denies any skin issues or rashes.\n",
            "\n",
            "3. Head: The patient denies any headaches or head injuries.\n",
            "\n",
            "4. Eyes: The patient denies any vision problems or eye pain.\n",
            "\n",
            "5. Ears, Nose, Throat: The patient denies any hearing issues, nasal congestion, or sore throat.\n",
            "\n",
            "6. Cardiovascular: The patient denies any heart problems or chest pain.\n",
            "\n",
            "7. Respiratory: The patient denies any breathing issues or cough.\n",
            "\n",
            "8. Gastrointestinal: The patient reports sharp abdominal pain under her right ribs and morning sickness. She denies any nausea or abdominal pain. She reports having had a miscarriage in the past and currently has three living children. She also denies any abdominal pain or discomfort while bending her knee or pushing against the doctor's hand.\n",
            "\n",
            "9. Genitourinary: The patient reports frequent urination every two to three hours during\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW CODE"
      ],
      "metadata": {
        "id": "NrOulgDGK_MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# RoSsummary class\n",
        "class RoSsummary(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=4096, num_output=512, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieve documents from the index using the query\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combine the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def map_phase(self, text_chunks):\n",
        "        intermediate_summaries = []\n",
        "        for chunk in text_chunks:\n",
        "            print(f\"Processing chunk: {chunk[:50]}...\")  # Debugging print statement\n",
        "            relevant_knowledge_base = self.search_index(chunk)\n",
        "            prompt_template = PromptTemplate(\n",
        "                input_variables=[\"transcript\", \"knowledge_base\"],\n",
        "                template=(\n",
        "                    \"Based on the provided clinical conversation and the Conversation Transcript, generate a detailed Review of Systems (RoS) for the patient based on the Conversation Transcript. \"\n",
        "                    \"The RoS should cover the following body systems and include specific details if mentioned, otherwise state 'denies any issues':\\n\\n\"\n",
        "                    \"1. General: Chief complaint\\n\"\n",
        "                    \"2. Skin: Any skin issues or rashes.\\n\"\n",
        "                    \"3. Head: Any headaches or head injuries.\\n\"\n",
        "                    \"4. Eyes: Vision problems or eye pain.\\n\"\n",
        "                    \"5. Ears, Nose, Throat: Hearing issues, nasal congestion, sore throat, etc.\\n\"\n",
        "                    \"6. Cardiovascular: Heart problems, chest pain, etc.\\n\"\n",
        "                    \"7. Respiratory: Breathing issues, cough, etc.\\n\"\n",
        "                    \"8. Gastrointestinal: Abdominal pain, nausea, etc.\\n\"\n",
        "                    \"9. Genitourinary: Urination problems, etc.\\n\"\n",
        "                    \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                    \"Relevant Knowledge Base:\\n{knowledge_base}\\n\\n\"\n",
        "                    \"Please ensure the RoS is comprehensive, clear, and formatted properly.\"\n",
        "                )\n",
        "            )\n",
        "            llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "            prompt = prompt_template.format(transcript=chunk, knowledge_base=relevant_knowledge_base)\n",
        "            try:\n",
        "                summary = llm(prompt)\n",
        "                print(f\"Generated summary for chunk: {summary[:50]}...\")  # Debugging print statement\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating summary for chunk: {chunk[:50]}...: {e}\")\n",
        "                summary = \"\"\n",
        "            intermediate_summaries.append(summary)\n",
        "        return intermediate_summaries\n",
        "\n",
        "    def reduce_phase(self, intermediate_summaries):\n",
        "        final_ros = {\n",
        "            \"General\": set(),\n",
        "            \"Skin\": set(),\n",
        "            \"Head\": set(),\n",
        "            \"Eyes\": set(),\n",
        "            \"Ears, Nose, Throat\": set(),\n",
        "            \"Cardiovascular\": set(),\n",
        "            \"Respiratory\": set(),\n",
        "            \"Gastrointestinal\": set(),\n",
        "            \"Genitourinary\": set()\n",
        "        }\n",
        "\n",
        "        for summary in intermediate_summaries:\n",
        "            print(f\"Processing intermediate summary: {summary[:50]}...\")  # Debugging print statement\n",
        "            sections = summary.split(\"\\n\\n\")\n",
        "            for section in sections:\n",
        "                if section.startswith(\"General:\"):\n",
        "                    final_ros[\"General\"].add(section.replace(\"General:\", \"\").strip())\n",
        "                elif section.startswith(\"Skin:\"):\n",
        "                    final_ros[\"Skin\"].add(section.replace(\"Skin:\", \"\").strip())\n",
        "                elif section.startswith(\"Head:\"):\n",
        "                    final_ros[\"Head\"].add(section.replace(\"Head:\", \"\").strip())\n",
        "                elif section.startswith(\"Eyes:\"):\n",
        "                    final_ros[\"Eyes\"].add(section.replace(\"Eyes:\", \"\").strip())\n",
        "                elif section.startswith(\"Ears, Nose, Throat:\"):\n",
        "                    final_ros[\"Ears, Nose, Throat\"].add(section.replace(\"Ears, Nose, Throat:\", \"\").strip())\n",
        "                elif section.startswith(\"Cardiovascular:\"):\n",
        "                    final_ros[\"Cardiovascular\"].add(section.replace(\"Cardiovascular:\", \"\").strip())\n",
        "                elif section.startswith(\"Respiratory:\"):\n",
        "                    final_ros[\"Respiratory\"].add(section.replace(\"Respiratory:\", \"\").strip())\n",
        "                elif section.startswith(\"Gastrointestinal:\"):\n",
        "                    final_ros[\"Gastrointestinal\"].add(section.replace(\"Gastrointestinal:\", \"\").strip())\n",
        "                elif section.startswith(\"Genitourinary:\"):\n",
        "                    final_ros[\"Genitourinary\"].add(section.replace(\"Genitourinary:\", \"\").strip())\n",
        "\n",
        "        combined_summary = (\n",
        "            \"Review of Systems (RoS):\\n\\n\"\n",
        "            f\"General: {' '.join(final_ros['General'])}\\n\\n\"\n",
        "            f\"Skin: {' '.join(final_ros['Skin'])}\\n\\n\"\n",
        "            f\"Head: {' '.join(final_ros['Head'])}\\n\\n\"\n",
        "            f\"Eyes: {' '.join(final_ros['Eyes'])}\\n\\n\"\n",
        "            f\"Ears, Nose, Throat: {' '.join(final_ros['Ears, Nose, Throat'])}\\n\\n\"\n",
        "            f\"Cardiovascular: {' '.join(final_ros['Cardiovascular'])}\\n\\n\"\n",
        "            f\"Respiratory: {' '.join(final_ros['Respiratory'])}\\n\\n\"\n",
        "            f\"Gastrointestinal: {' '.join(final_ros['Gastrointestinal'])}\\n\\n\"\n",
        "            f\"Genitourinary: {' '.join(final_ros['Genitourinary'])}\"\n",
        "        )\n",
        "        return combined_summary\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['alternatives'][0]['content'] for item in transcript_data['results']['items'] if 'alternatives' in item and item['alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Splitting the transcript into chunks for Map phase\n",
        "        text_chunks = [transcript_text[i:i+2000] for i in range(0, len(transcript_text), 2000)]\n",
        "        print(f\"Text chunks: {len(text_chunks)}\")  # Debugging print statement\n",
        "\n",
        "        # Map phase: generate intermediate summaries for each chunk\n",
        "        intermediate_summaries = self.map_phase(text_chunks)\n",
        "        print(f\"Intermediate summaries: {len(intermediate_summaries)}\")  # Debugging print statement\n",
        "\n",
        "        # Reduce phase: combine intermediate summaries into final summary\n",
        "        final_summary = self.reduce_phase(intermediate_summaries)\n",
        "        return final_summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = RoSsummary(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/June12-01_AbdominalPain.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPoF1fWDa0_p",
        "outputId": "1c3cb078-451d-426e-ce7c-ee08cf9029da"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-53-5b307f4ed379>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text chunks: 4\n",
            "Processing chunk: Hi , I'm doctor . This is my guy , Jake . Can you ...\n",
            "Generated summary for chunk: \n",
            "\n",
            "Review of Systems (RoS):\n",
            "\n",
            "1. General:\n",
            "- Chief co...\n",
            "Processing chunk: . So you have three . Ok . Thanks . Um , any other...\n",
            "Generated summary for chunk: \n",
            "\n",
            "Review of Systems (RoS) for the Patient:\n",
            "\n",
            "1. Gen...\n",
            "Processing chunk: gn . She has a negative heel strike . She has , uh...\n",
            "Generated summary for chunk: \n",
            "\n",
            "Review of Systems (RoS) for Patient:\n",
            "\n",
            "1. General...\n",
            "Processing chunk: ain medication can make people nauseated . Ok . Um...\n",
            "Generated summary for chunk: \n",
            "Review of Systems (RoS) for the patient:\n",
            "\n",
            "1. Gene...\n",
            "Intermediate summaries: 4\n",
            "Processing intermediate summary: \n",
            "\n",
            "Review of Systems (RoS):\n",
            "\n",
            "1. General:\n",
            "- Chief co...\n",
            "Processing intermediate summary: \n",
            "\n",
            "Review of Systems (RoS) for the Patient:\n",
            "\n",
            "1. Gen...\n",
            "Processing intermediate summary: \n",
            "\n",
            "Review of Systems (RoS) for Patient:\n",
            "\n",
            "1. General...\n",
            "Processing intermediate summary: \n",
            "Review of Systems (RoS) for the patient:\n",
            "\n",
            "1. Gene...\n",
            "Review of Systems (RoS):\n",
            "\n",
            "General: \n",
            "\n",
            "Skin: \n",
            "\n",
            "Head: \n",
            "\n",
            "Eyes: \n",
            "\n",
            "Ears, Nose, Throat: \n",
            "\n",
            "Cardiovascular: \n",
            "\n",
            "Respiratory: \n",
            "\n",
            "Gastrointestinal: \n",
            "\n",
            "Genitourinary: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#PE"
      ],
      "metadata": {
        "id": "rcBYaQwtX3uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# TestNameExtractor class for extracting test names\n",
        "class TestNameExtractor(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=2048, num_output=256, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieving documents from the index\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combining the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['alternatives'][0]['content'] for item in transcript_data['results']['items'] if 'alternatives' in item and item['alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        prompt_template = PromptTemplate(\n",
        "            input_variables=[\"transcript\"],\n",
        "            template=(\n",
        "                \"The following is a clinical conversation between a doctor and a patient. \"\n",
        "                \"Based on this conversation transcript, extract only the names of the tests explicitly mentioned by the doctor during the examination. \"\n",
        "                \"Do not include any medications, treatments, or irrelevant information; only list the tests.\\n\\n\"\n",
        "                \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                \"Please list only the test names explicitly mentioned by the doctor as it is in the Conversation Transcript, excluding any medications, treatments, or irrelevant information.\"\n",
        "            )\n",
        "        )\n",
        "\n",
        "        # Generating the summary using the LLM\n",
        "        llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "        prompt = prompt_template.format(transcript=transcript_text)\n",
        "        summary = llm(prompt)\n",
        "\n",
        "        return summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = TestNameExtractor(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/June12-01_AbdominalPain.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atyyd6bNkZeH",
        "outputId": "9637e8cf-2623-4fa9-8306-342417db0898"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-13d35fa8172e>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "1. CBCL FTS\n",
            "2. Beta quant level\n",
            "3. Type in Rh\n",
            "4. Pelvic exam\n",
            "5. Wet prep\n",
            "6. STD screen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NEW CODE"
      ],
      "metadata": {
        "id": "lSSz-roSahl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import pandas as pd\n",
        "from llama_index.core import Document, GPTVectorStoreIndex, PromptHelper, ServiceContext\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "\n",
        "# OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# TestNameExtractor class for extracting test names\n",
        "class TestNameExtractor(AbstractSummarizer):\n",
        "    def __init__(self, knowledge_base_path):\n",
        "        self.knowledge_base_path = knowledge_base_path\n",
        "        self.knowledge_base = self.load_knowledge_base()\n",
        "        self.index = self.create_index()\n",
        "\n",
        "    def load_knowledge_base(self):\n",
        "        # Loading the CSV file as a string\n",
        "        df = pd.read_csv(self.knowledge_base_path)\n",
        "        return df.to_string()\n",
        "\n",
        "    def create_index(self):\n",
        "        # Creating a document list\n",
        "        documents = [Document(text=self.knowledge_base)]\n",
        "\n",
        "        # Initializing required components for ServiceContext\n",
        "        prompt_helper = PromptHelper(chunk_size_limit=2048, num_output=256, chunk_overlap_ratio=0.2)\n",
        "        service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n",
        "\n",
        "        # Creating and returning the index\n",
        "        return GPTVectorStoreIndex.from_documents(documents, service_context=service_context)\n",
        "\n",
        "    def search_index(self, query):\n",
        "        # Searching for relevant documents in the index\n",
        "        results = self.retrieve_relevant_documents(query)\n",
        "        return results\n",
        "\n",
        "    def retrieve_relevant_documents(self, query):\n",
        "        # Retrieving documents from the index\n",
        "        retriever = self.index.as_retriever(verbose=True)\n",
        "        search_results = retriever.retrieve(query)\n",
        "\n",
        "        # Combining the texts of the top 5 results\n",
        "        return \"\\n\".join([result.text.replace(\"Patient\", \"Other Patient\") for result in search_results[:5]])\n",
        "\n",
        "    def map_phase(self, text_chunks):\n",
        "        intermediate_summaries = []\n",
        "        for chunk in text_chunks:\n",
        "            prompt_template = PromptTemplate(\n",
        "                input_variables=[\"transcript\"],\n",
        "                template=(\n",
        "                    \"The following is a clinical conversation between a doctor and a patient. \"\n",
        "                    \"Based on this conversation transcript, extract only the names of the tests explicitly mentioned by the doctor during the examination. \"\n",
        "                    \"Do not include any medications, treatments, or irrelevant information; only list the test names exactly as mentioned.\\n\\n\"\n",
        "                    \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                    \"Please list only the test names explicitly mentioned by the doctor as it is in the Conversation Transcript, excluding any medications, treatments, or irrelevant information.\"\n",
        "                )\n",
        "            )\n",
        "            llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "            prompt = prompt_template.format(transcript=chunk)\n",
        "            try:\n",
        "                summary = llm(prompt)\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating summary for chunk: {chunk[:50]}...: {e}\")\n",
        "                summary = \"\"\n",
        "            intermediate_summaries.append(summary)\n",
        "        return intermediate_summaries\n",
        "\n",
        "    def reduce_phase(self, intermediate_summaries):\n",
        "        test_names = set()\n",
        "        for summary in intermediate_summaries:\n",
        "            for line in summary.split(\"\\n\"):\n",
        "                line = line.strip()\n",
        "                if line and not any(keyword in line.lower() for keyword in [\"medication\", \"treatment\", \"irrelevant\"]):\n",
        "                    test_names.add(line)\n",
        "\n",
        "        combined_summary = \"Tests explicitly mentioned by the doctor:\\n\" + \"\\n\".join(test_names)\n",
        "        return combined_summary\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['alternatives'][0]['content'] for item in transcript_data['results']['items'] if 'alternatives' in item and item['alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Splitting the transcript into chunks for Map phase\n",
        "        text_chunks = [transcript_text[i:i+2000] for i in range(0, len(transcript_text), 2000)]\n",
        "\n",
        "        # Map phase: generate intermediate summaries for each chunk\n",
        "        intermediate_summaries = self.map_phase(text_chunks)\n",
        "\n",
        "        # Reduce phase: combine intermediate summaries into final summary\n",
        "        final_summary = self.reduce_phase(intermediate_summaries)\n",
        "        return final_summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path, knowledge_base_path):\n",
        "    # Generating the summary\n",
        "    summarizer = TestNameExtractor(knowledge_base_path)\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/June12-01_AbdominalPain.json'\n",
        "knowledge_base_path = '/content/MTS-Dialog-TestSet-1-MEDIQA-Chat-2023.csv'\n",
        "summary = execute(transcript_path, knowledge_base_path)\n",
        "print(summary)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2h3SFYp5dx5d",
        "outputId": "9963a026-5853-465a-c5ac-821c72f84ca4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-567b26793212>:40: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
            "  service_context = ServiceContext.from_defaults(prompt_helper=prompt_helper)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tests explicitly mentioned by the doctor:\n",
            "6. Flank tenderness\n",
            "-Beta quant level\n",
            "3. Rebound\n",
            "5. CVA\n",
            "-CBCL FT S\n",
            "2. Heel strike\n",
            "- Labs\n",
            "-Pelvic exam\n",
            "-Type in Rh\n",
            "-STD screen\n",
            "3. Ultrasound\n",
            "2. STD testing\n",
            "1. Abdominal ultrasound\n",
            "4. Guarding\n",
            "4. Prior pregnancies\n",
            "-Wet prep\n",
            "5. Miscarriage\n",
            "- Pelvic exam\n",
            "1. Murphy sign\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Assessment Plan"
      ],
      "metadata": {
        "id": "PoPBbV6yfqVc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from abc import ABC, abstractmethod\n",
        "import json\n",
        "import os\n",
        "from langchain import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"sk-proj-HwFPBND26SnyzXe8nuXgT3BlbkFJ4CQ71wBNBpziW0et7UGh\"\n",
        "\n",
        "# Abstract summarizer class\n",
        "class AbstractSummarizer(ABC):\n",
        "    @abstractmethod\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def summarize(self, transcript_path):\n",
        "        pass\n",
        "\n",
        "# AssessmentPlanExtractor class\n",
        "class AssessmentPlanExtractor(AbstractSummarizer):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def map_phase(self, text_chunks):\n",
        "        intermediate_summaries = []\n",
        "        for chunk in text_chunks:\n",
        "            prompt_template = PromptTemplate(\n",
        "                input_variables=[\"transcript\"],\n",
        "                template=(\n",
        "                    \"Based on the provided clinical conversation between a doctor and a patient, generate a detailed Assessment Plan. \"\n",
        "                    \"The Assessment Plan should include only the doctor's assessment and plan, clearly separated into two sections: Assessment and Plan. \"\n",
        "                    \"Each section should be detailed and follow the structure provided in the conversation.\\n\\n\"\n",
        "                    \"Conversation Transcript:\\n{transcript}\\n\\n\"\n",
        "                    \"Assessment:\\n\"\n",
        "                    \"Provide a detailed assessment of the patient's Chief complaint based on the Conversation Transcript.\\n\\n\"\n",
        "                    \"Plan:\\n\"\n",
        "                    \"1. List only the results of diagnostic tests given to the patient mentioned in the Transcript Conversation.\\n\"\n",
        "                    \"2. Include any relevant diagnostic results, such as EKG findings, if mentioned in the Transcript Conversation.\\n\"\n",
        "                    \"3. Mention the diagnosis and follow-up instructions provided by the doctor in the Transcript Conversation.\\n\"\n",
        "                    \"Include any medications or treatments only if mentioned explicitly in the transcript.\\n\\n\"\n",
        "                    \"Please ensure the assessment plan is comprehensive, clear, and formatted properly.\"\n",
        "                )\n",
        "            )\n",
        "            llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "            prompt = prompt_template.format(transcript=chunk)\n",
        "            try:\n",
        "                summary = llm(prompt)\n",
        "                print(f\"Generated summary for chunk: {summary}\")  # Debugging line\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating summary for chunk: {chunk[:50]}...: {e}\")\n",
        "                summary = \"\"\n",
        "            intermediate_summaries.append(summary)\n",
        "        return intermediate_summaries\n",
        "\n",
        "    def reduce_phase(self, intermediate_summaries):\n",
        "        final_assessment = []\n",
        "        final_plan = []\n",
        "\n",
        "        for summary in intermediate_summaries:\n",
        "            sections = summary.split(\"Plan:\\n\")\n",
        "            if len(sections) == 2:\n",
        "                assessment, plan = sections\n",
        "                assessment = assessment.replace(\"Assessment:\\n\", \"\").strip()\n",
        "                plan = plan.strip()\n",
        "                final_assessment.append(assessment)\n",
        "                final_plan.append(plan)\n",
        "\n",
        "        combined_summary = (\n",
        "            \"Assessment:\\n\\n\" + \"\\n\\n\".join(final_assessment) + \"\\n\\nPlan:\\n\\n\" + \"\\n\\n\".join(final_plan)\n",
        "        )\n",
        "        return combined_summary\n",
        "\n",
        "    def summarize(self, transcript_path):\n",
        "        # Loading the transcript JSON file\n",
        "        with open(transcript_path, 'r') as file:\n",
        "            transcript_data = json.load(file)\n",
        "\n",
        "        # Extracting the conversation text\n",
        "        docs = [item['alternatives'][0]['content'] for item in transcript_data['results']['items'] if 'alternatives' in item and item['alternatives']]\n",
        "        transcript_text = \" \".join(docs)\n",
        "\n",
        "        # Splitting the transcript into chunks for Map phase\n",
        "        text_chunks = [transcript_text[i:i+2000] for i in range(0, len(transcript_text), 2000)]\n",
        "\n",
        "        # Map phase: generate intermediate summaries for each chunk\n",
        "        intermediate_summaries = self.map_phase(text_chunks)\n",
        "\n",
        "        # Reduce phase: combine intermediate summaries into final summary\n",
        "        final_summary = self.reduce_phase(intermediate_summaries)\n",
        "        return final_summary\n",
        "\n",
        "# Execute function\n",
        "def execute(transcript_path):\n",
        "    # Generating the summary\n",
        "    summarizer = AssessmentPlanExtractor()\n",
        "    return summarizer.summarize(transcript_path)\n",
        "\n",
        "# Example usage\n",
        "transcript_path = '/content/June12-01_AbdominalPain.json'  # Change this to your transcript path\n",
        "summary = execute(transcript_path)\n",
        "print(summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVW2jYEPghya",
        "outputId": "b002d439-6cf8-4767-aff6-ef8ffde104ae"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated summary for chunk: \n",
            "\n",
            "Assessment:\n",
            "Jake has presented to the doctor's office with a chief complaint of sharp abdominal pain located under his right ribs. He is currently nine weeks pregnant and is experiencing morning sickness, lower back pain, and the abdominal pain. He denies any bleeding, discharge, or concerns for STDs. He has not had any vomiting other than the normal morning sickness. He has not seen an OB/GYN yet and has not had an ultrasound or any prior problems with his previous pregnancies.\n",
            "\n",
            "Plan:\n",
            "1. Jake will be scheduled for an ultrasound to assess the status of his pregnancy and to rule out any possible complications.\n",
            "2. He will also undergo a urine test for STDs to rule out any potential infections.\n",
            "3. An EKG will be done to evaluate his heart function and rule out any cardiac issues that may be causing his abdominal pain.\n",
            "4. The doctor will also order a complete blood count (CBC) and a comprehensive metabolic panel (CMP) to assess his overall health and rule out any underlying conditions.\n",
            "5. Based on the results of these diagnostic tests, the doctor will provide a definitive diagnosis and formulate a treatment plan.\n",
            "6. If the abdominal pain is determined to be related to the pregnancy, the doctor will recommend rest and increased fluid intake.\n",
            "7. If the\n",
            "Generated summary for chunk: \n",
            "\n",
            "Assessment:\n",
            "The patient presents with a chief complaint of right upper quadrant tenderness and a positive Murphy sign. She reports no other symptoms and denies any recent exposure to sick individuals. She is currently pregnant and has no other known medical problems.\n",
            "\n",
            "Plan:\n",
            "1. The patient will undergo a full physical examination, including a focused abdominal examination.\n",
            "2. Diagnostic tests will include a complete blood count (CBC) and liver function tests (LFTs) to assess for any underlying conditions.\n",
            "3. If indicated, an ultrasound or CT scan may be ordered to further evaluate the patient's symptoms.\n",
            "4. The patient will be instructed to follow up with her primary care physician for further management.\n",
            "5. The patient will be advised to rest and avoid strenuous activities until her symptoms improve.\n",
            "6. If any pain is present, the patient may take over-the-counter pain medication as needed.\n",
            "7. The patient will be educated on the importance of maintaining a healthy diet and avoiding fatty or greasy foods.\n",
            "8. The patient will be advised to monitor her symptoms and report any changes or worsening of her condition.\n",
            "9. If the patient's symptoms persist or worsen, she will be instructed to seek immediate medical attention.\n",
            "10. Follow-up appointments will be scheduled as needed to monitor the patient\n",
            "Generated summary for chunk: \n",
            "\n",
            "Assessment:\n",
            "The patient is a pregnant female with complaints of abdominal pain in the right upper quadrant. Upon physical examination, she has a negative heel strike and tenderness in the affected area with no rebound or guarding. Her lungs and heart are regular, and there is no evidence of CVA or flank tenderness. Based on her symptoms and history, further evaluation is necessary to determine the cause of her abdominal pain.\n",
            "\n",
            "Plan:\n",
            "1. Diagnostic tests:\n",
            "- CBC with differential to check for any signs of infection or anemia.\n",
            "- Beta quant level to assess the patient's pregnancy hormone levels.\n",
            "- Type and Rh to determine the patient's blood type, which is crucial for pregnancy.\n",
            "- Pelvic examination to assess the condition of the reproductive organs.\n",
            "- Wet prep to check for any vaginal infections.\n",
            "- STD screen to rule out any sexually transmitted infections.\n",
            "\n",
            "2. Diagnosis:\n",
            "Based on the patient's symptoms and history, the doctor suspects that the abdominal pain could be due to a possible infection or complication related to her pregnancy. Further tests and examinations are needed to confirm the diagnosis.\n",
            "\n",
            "3. Follow-up instructions:\n",
            "The patient will be monitored closely for any changes in her condition. She will be advised to follow-up with her OB-GYN for routine prenatal care and to discuss the findings\n",
            "Generated summary for chunk: \n",
            "\n",
            "Assessment:\n",
            "The patient is experiencing nausea, which could be a side effect of the pain medication. The pain is located in the right upper quadrant, which could indicate an underlying condition. A pelvic exam will be performed to further assess the cause of the pain.\n",
            "\n",
            "Plan:\n",
            "1. Diagnostic tests:\n",
            "- Pelvic exam: to assess for any abnormalities in the reproductive organs.\n",
            "- Blood tests: to check for any signs of infection or other underlying conditions.\n",
            "- Urine analysis: to rule out any urinary tract infections.\n",
            "- Imaging tests (e.g. ultrasound, CT scan): to evaluate the structures in the right upper quadrant.\n",
            "\n",
            "2. Relevant diagnostic results:\n",
            "- EKG findings: if any abnormalities are detected, further tests will be ordered to assess cardiac function.\n",
            "\n",
            "3. Diagnosis and follow-up instructions:\n",
            "- The patient will be diagnosed based on the results of the diagnostic tests.\n",
            "- Follow-up instructions will be provided based on the diagnosis and the severity of the condition.\n",
            "- If any underlying conditions are detected, appropriate treatment will be initiated and the patient will be advised on the next steps.\n",
            "\n",
            "Medications:\n",
            "- Dilaudid 0.5 mg: to manage the patient's pain in the right upper quadrant.\n",
            "- Zofran 4 mg IV: to alleviate the\n",
            "Assessment:\n",
            "\n",
            "Jake has presented to the doctor's office with a chief complaint of sharp abdominal pain located under his right ribs. He is currently nine weeks pregnant and is experiencing morning sickness, lower back pain, and the abdominal pain. He denies any bleeding, discharge, or concerns for STDs. He has not had any vomiting other than the normal morning sickness. He has not seen an OB/GYN yet and has not had an ultrasound or any prior problems with his previous pregnancies.\n",
            "\n",
            "The patient presents with a chief complaint of right upper quadrant tenderness and a positive Murphy sign. She reports no other symptoms and denies any recent exposure to sick individuals. She is currently pregnant and has no other known medical problems.\n",
            "\n",
            "The patient is a pregnant female with complaints of abdominal pain in the right upper quadrant. Upon physical examination, she has a negative heel strike and tenderness in the affected area with no rebound or guarding. Her lungs and heart are regular, and there is no evidence of CVA or flank tenderness. Based on her symptoms and history, further evaluation is necessary to determine the cause of her abdominal pain.\n",
            "\n",
            "The patient is experiencing nausea, which could be a side effect of the pain medication. The pain is located in the right upper quadrant, which could indicate an underlying condition. A pelvic exam will be performed to further assess the cause of the pain.\n",
            "\n",
            "Plan:\n",
            "\n",
            "1. Jake will be scheduled for an ultrasound to assess the status of his pregnancy and to rule out any possible complications.\n",
            "2. He will also undergo a urine test for STDs to rule out any potential infections.\n",
            "3. An EKG will be done to evaluate his heart function and rule out any cardiac issues that may be causing his abdominal pain.\n",
            "4. The doctor will also order a complete blood count (CBC) and a comprehensive metabolic panel (CMP) to assess his overall health and rule out any underlying conditions.\n",
            "5. Based on the results of these diagnostic tests, the doctor will provide a definitive diagnosis and formulate a treatment plan.\n",
            "6. If the abdominal pain is determined to be related to the pregnancy, the doctor will recommend rest and increased fluid intake.\n",
            "7. If the\n",
            "\n",
            "1. The patient will undergo a full physical examination, including a focused abdominal examination.\n",
            "2. Diagnostic tests will include a complete blood count (CBC) and liver function tests (LFTs) to assess for any underlying conditions.\n",
            "3. If indicated, an ultrasound or CT scan may be ordered to further evaluate the patient's symptoms.\n",
            "4. The patient will be instructed to follow up with her primary care physician for further management.\n",
            "5. The patient will be advised to rest and avoid strenuous activities until her symptoms improve.\n",
            "6. If any pain is present, the patient may take over-the-counter pain medication as needed.\n",
            "7. The patient will be educated on the importance of maintaining a healthy diet and avoiding fatty or greasy foods.\n",
            "8. The patient will be advised to monitor her symptoms and report any changes or worsening of her condition.\n",
            "9. If the patient's symptoms persist or worsen, she will be instructed to seek immediate medical attention.\n",
            "10. Follow-up appointments will be scheduled as needed to monitor the patient\n",
            "\n",
            "1. Diagnostic tests:\n",
            "- CBC with differential to check for any signs of infection or anemia.\n",
            "- Beta quant level to assess the patient's pregnancy hormone levels.\n",
            "- Type and Rh to determine the patient's blood type, which is crucial for pregnancy.\n",
            "- Pelvic examination to assess the condition of the reproductive organs.\n",
            "- Wet prep to check for any vaginal infections.\n",
            "- STD screen to rule out any sexually transmitted infections.\n",
            "\n",
            "2. Diagnosis:\n",
            "Based on the patient's symptoms and history, the doctor suspects that the abdominal pain could be due to a possible infection or complication related to her pregnancy. Further tests and examinations are needed to confirm the diagnosis.\n",
            "\n",
            "3. Follow-up instructions:\n",
            "The patient will be monitored closely for any changes in her condition. She will be advised to follow-up with her OB-GYN for routine prenatal care and to discuss the findings\n",
            "\n",
            "1. Diagnostic tests:\n",
            "- Pelvic exam: to assess for any abnormalities in the reproductive organs.\n",
            "- Blood tests: to check for any signs of infection or other underlying conditions.\n",
            "- Urine analysis: to rule out any urinary tract infections.\n",
            "- Imaging tests (e.g. ultrasound, CT scan): to evaluate the structures in the right upper quadrant.\n",
            "\n",
            "2. Relevant diagnostic results:\n",
            "- EKG findings: if any abnormalities are detected, further tests will be ordered to assess cardiac function.\n",
            "\n",
            "3. Diagnosis and follow-up instructions:\n",
            "- The patient will be diagnosed based on the results of the diagnostic tests.\n",
            "- Follow-up instructions will be provided based on the diagnosis and the severity of the condition.\n",
            "- If any underlying conditions are detected, appropriate treatment will be initiated and the patient will be advised on the next steps.\n",
            "\n",
            "Medications:\n",
            "- Dilaudid 0.5 mg: to manage the patient's pain in the right upper quadrant.\n",
            "- Zofran 4 mg IV: to alleviate the\n"
          ]
        }
      ]
    }
  ]
}